"\nверни json {\n\"title\": короткий кликбейт заголовок-мнение для текста на русском языке,\n\n\"description\": главные тезисы текста в художественно-публицистическом жанре с использованием одного образного сравнения длиной 2 предложения по 15 слов на русском языке, \n\n\"morality\": веселый, оптимистический  совет на основе вывода из текста на русском языке длиной 1 предложение\n\n},  \n\nтекст -\n Moralities of Intelligent Machines is a project that investigates people’s attitudes towards moral choices made by artificial intelligence.  In the latest study completed under the project, study participants read short narratives where either a robot, a somewhat humanoid robot known as iRobot, a robot with a strong humanoid appearance called iClooney or a human being encounters a moral problem along the lines of the trolley dilemma, making a specific decision. The participants were also shown images of these agents, after which they assessed the morality of their decisions. The trolley dilemma is a problem where a person sees a trolley careening on the tracks, without anyone in control, towards five people. The person can either do nothing or turn the trolley onto another track, saving the five people but killing another individual on the other track. According to the study, people consider the choice made by the humanoid iRobot and iClooney less ethically sound than the same decision made by a human and a robot with a traditional robot-like appearance. Michael Laakasuo, a senior researcher at the University of Helsinki, project lead and the principal investigator of the study, links the findings to the uncanny valley effect, which has been identified in prior research. “Humanness in artificial intelligence is perceived as eerie or creepy, and attitudes towards such robots are more negative than towards more machine-like robots. This may be due to, for example, the difficulty of reacting to a humanoid being: is it an animal, a human or a tool?” According to Laakasuo, the findings indicate that humans do not find robots making moral decisions a strange idea, since the decisions made by a human and a traditional robot were seen as equally acceptable. Instead, the appearance of the robot makes a difference to evaluating their morality. Discussion guides the regulation of AI Laakasuo says that the number of intelligent machines making moral choices is growing in our society, with self-driving cars as an example. “It’s important to know how people view intelligent machines and what kinds of factors affect related moral assessment. For instance, are traffic violations perpetrated by a stylish self-driving car perceived differently from those of a less classy model?” This knowledge can influence the direction of AI and robotics development, as well as, among other things, product branding. Knowledge can also shape the political discussion relating to the regulation of artificial intelligence. For example, self-driving cars can become test laboratories of sorts for private companies: in the case of accidents, the consequences can be dealt with using money, risking human health in the name of technological advancement with appeals to consequentialist morals. “What kind of robots do we want to have among us: robots who save five people from being run over by a trolley, sacrificing one person, or robots who refuse to sacrifice anyone even if it would mean saving several lives? Should robots be designed to look like humans or not if their appearance affects the perceived morality of their actions?” Source: University of HelsinkiContact: Michael Laakasuo – University of HelsinkiImage: The image is in the public domain Artificial intelligence and robotics are rapidly advancing. Humans are increasingly often affected by autonomous machines making choices with moral repercussions. At the same time, classical research in robotics shows that people are adverse to robots that appear eerily human—a phenomenon commonly referred to as the uncanny valley effect. Yet, little is known about how machines’ appearances influence how human evaluate their moral choices. Here we integrate the uncanny valley effect into moral psychology. In two experiments we test whether humans evaluate identical moral choices made by robots differently depending on the robots’ appearance. Participants evaluated either deontological (“rule based”) or utilitarian (“consequence based”) moral decisions made by different robots. The results provide first indication that people evaluate moral choices by robots that resemble humans as less moral compared to the same moral choices made by humans or non-human robots: a moral uncanny valley effect. We discuss the implications of our findings for moral psychology, social robotics and AI-safety policy."